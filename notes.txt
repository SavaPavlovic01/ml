HOUSING (REGRESSION)
---------------------------------------------------------------------------------------------
prvo si napisao kod koji skida i unzipuje (ovo nije bas bitno iskr)

onda mozes da pogledas histograme sa .hist(bins=neki broj, figsize=(x,y)) (ovo bins je koliko onih kao kofa ima)
i mozes da pozoves .info i .describe da vidis sta i kako

onda treba da podelis na test i train set (mozes rucno al bolje da samo pozoves ugradjenu f-ju)
onda je radio stratified podelu (ovo se radi da bi zastupljenost pojedinih grupa bila ista u svakom setu kao sto je u celoj bazi) 
---data["income_cat"] = pd.cut(data["median_income"], bins=[0, 1.5, 3, 4.5, 6, np.inf], labels=[1,2,3,4,5])--- (OVO DELI U GRUPE)
na osnovu vrednosti u bins dodeljuje labelu iz labels
onda pozoves StratifiedShuffleSplit da bi dobio indekse

matrix = data.corr() da bi dobio koeficijent korelacije
pa matrix["sta god"] da bi dobio korelaciju s tim
scatter_matrix i tu prosledis kolone i on ti nacrta scatter plot svaki sa svakim
mozes i data.plot(kind = "scatter"....) da bi dobio samo korelaciju 2 kolone

posle ovoga mozes da dodas neke svoje kolone

mozes da razdvoji podatke od labela, jer kad transformises podatke vrv neces da radis iste stvari sa labelama

kad neka polja fale, mozes ili da izbaci taj red, celu kolonu ili da stavis neku vrednost tu (kao npr null, mean, median...)

kod sklearn ako hoces custom transformacije napravi klasu koja nasledjuje od BaseEstimator i TransformerMixin pa samo napises
fit (koji vraca self uvek) i transform

ako neko polje ima tekst, mozes ili ordinalEncoder ako ima smisla da se porede ili OneHot (svaka razlicita vrednost postje polje)

kad skaliras: normalizacija (min-max scaling) ili standardization. Outliers manje uticu na ovo drugo, ako alogiratm prihvata samo
vrednosti izmedju 0 i 1 moras ovo prvo, jer drugo daje izvan ovog opsega 

Pipeline(Prosljedjujes mu tuple(ime, transformator)) i ColumnTransformer(Tuple(ime, transformator i kolone na kojima treba da radi))

Sracunaj gresku pa isprobaj razlicite modele(algoritme) i vidi koj ima najmanju gresku

Napravi validation set (cross evaluation), ovako proveravs da li overfit-uje
Kad bude dobar i za validatio set mozes da ga treniras na trainSet + validationSet

Odredi par modela koji najbolje rade

GridSearchCV trazi najbolje hiper-parametre, RandomSearchCV se koristi ako ima previse kombinacija  

Kod RandomForest mozes da vidis koliko je koji podatak bitan, onda mozda mozes da izbacis manje bitne podatke

95% confidance interval(idk sta je ovo)

MNIST (classification)
----------------------------------------------------------------------------------------------------------------
Cross validation: podelis na K fold-ova (grupe podjednake velicine) pa ga treniras na K-1 fold-ova i testiras na onaj koji je osta.
Ovo ponavljas K puta, svaki put izostavis razlicit fold

Cross_val_score radi cross evaluation. Vraca preciznost
cross_val_predict: isto cross evaluation ali vraca predikcije

Kod klasifikacije tacnost nije toliko bitna
Confusion matrix (koliko puta je clan klase B klasifikovan kao clan klase A)

precision i recall (true positive rate TPR)
f1_score kombinacija ove dve, korisno ako treba da uporedjujes modele

kad je manji recall to znaci da ima dosta false negative

recall i precision su obrnuto proporcijalni, zavise od threshold-a
mozes da pravis gravik zavisnosti recall-a od precision-a (precision_recall_curve u sklearn)

receiver operating characterisitc (ROC) curve
grafik recall (true positive rate) vs false positive rate = 1 - true negative rate (specificity)
sto je povrsina ispod krive bliza 1 to bolje

prvu krivu koristis ako te vise zanima FP ili ako je pozitivna klasa retka

ako alogoritam moze da radi samo binarnu klasifikaciju, mozes da treniras K modela
npr ako treba da razlikuje brojeve, svaki model prepoznaje da li je slika taj neki broj ili ne
onda uzmes score koji su dodelili o uzmes onaj koji je najveci (One-versus-all OvA)

ili mozes model koji razlikuje svaka dva broja(da li je 0 ili 1, 0 ili 2....) one versus one (Ovo) strategija
onda kledas koja klasa je dobila najvise "duela"

Confusion matrica mozes da prikazes sa matplotlib.matshow

mozda je dobra ideja da na pocetku razdvojis kolone koje su brojevi i kolone koje su kategorije


LINEAR REGRESSION
-------------------------------------------------------------------------------------------------------------------------------------
predicted_value = O_0 + O_1 * X_1 + .... + O_n * X_n
O_0 je bias, ostali su weights
predicted_value (y sa kapicom u knjizi) = O*X (vektori) (O sadrzi sve one O vrednosti, X na pocetku ima 1)

hoces da minimizujes gresku . 2 nacina:
Normal equation: O_min = (X^T * X)^-1 * X^T * y
SVD: O_min = X^+ * y (ovo X+ je pseudoinvers)
pseudoinvers se racuna tako sto uradis SVD X = U * E * V^T, pa X^+ = V * E^+ * U^T
E+ se racuna tako sto sve mnogo male vrednosti stavis da su jednake nuli, i uradis inverz ostalih vrednosti i onda uradis transponovanje

slozenost:
Normal equation O(n^2.4 ili n^3) n je broj feature-a
SVD O(n^2)

kad se istrenira, brzo daje predikcije 

dobro rade sa velkim training set-om O(n)

Ova 2 nisu pogodna ako ima mnogo feature-a

Gradient descent
-----------------------------------------------------------------
MSE ima samo jedan minimun (nece nikad da se zaglavi na neki lokalan minimum)
bolje radi ako su sve kolone normalizovane, odnosno ako imaju isti scale

parametar learning rate: odredjuje velicinu "koraka", ako je previse malo treba dugo da konvergira, ako previse veliko divergira

tolerance: sproovodi algoritam sve dok norm(ono cudo sa parc izvodima valjda??) ne padne ispod tolerance, onda prekida

Batch gradient descent
-----------------------
suma parcijalnih izvoda, u svakom koraku radi sa celim set-om, lose ako imas veliki set
dobro skalira sa brojem feature-a
∂/∂θj (MSE(θ)) =2/m * ∑(θ^T * x − y) * x_j ovo je za samo jednu promenjivu

za sve zajedno dobijes ∇θ MSE(θ) = 2/m * X^T * (X*θ - y)
θ_next_step = θ - ni * ovo gore  (ni je learning rate)

Stochastic gradient descent
-----------------------------
umesto da uvek radi sa celim set-om, nasumicno izabere jednu instancu i radi korak samo s njom
mnogo dobro skalira sa velikim set-om
ne ide uvek pravo dole kao prosli, ovo moze da bude i dobro, jer moze da preskoci lokalani minimum
jedna iteracija = epoch
posle svake iteracije treba da se uradi shuffle, jer ti random uzorci moraju da imaju istu distribuciju kao data set
umesto konstantne learning-rate moze da se menja, tako da na pocetku bude veci

ako sam dobro razumeo:
ako ima n epoha i data set je velicine m ti radis
for epoch:
    for m:
        onda ovde random izaberes jednu instancu i pravis se kao da je to ceo set (m = 1)
        i dalje radis kao batch grad. descent
        ubrzanje se dobija kad radis onaj dot product. Jer u proslom su ti matrice velicine 1xm a ovde 1x1

Mini-batch gradient descent
----------------------------
ista fora kao prosli, samo sto umesto jedan imas batch podataka(>1)
ovo se cak i brze racuna zbog optimizacija vektorskih operaija (narucito ako koristi GPU)

|-------------------------------------------------------------------------------------------------------------|
|Algorithm       | Large m | Out-of-core support | Large n | Hyperparams | Scaling required | Scikit-Learn    |
|-------------------------------------------------------------------------------------------------------------|
|Normal Equation | Fast    | No                  | Slow    | 0           | No               | n/a             |
|SVD             | Fast    | No                  | Slow    | 0           | No               | LinearRegression|
|Batch GD        | Slow    | No                  | Fast    | 2           | Yes              | SGDRegressor    |
|Stochastic GD   | Fast    | Yes                 | Fast    | ≥2          | Yes              | SGDRegressor    |
|Mini-batch GD   | Fast    | Yes                 | Fast    | ≥2          | Yes              | SGDRegressor    |
|-------------------------------------------------------------------------------------------------------------|


Polynomial regression
--------------------------------------
mozes da dodas stepen feature-a kao novi feature i onda da samo koristis linear regression
npr ako imas 2 feature-a a i b, dodas a^2, a^3, b^2 ..., mozes cak da dodas a*b, a^2*b itd
ako ovako radis broj feature-a raste O(n!)
u sklearn samo pozoves PolynomialFeatures(degree)
ako stavis preveliki degree: overfitting
premali underfitting

mozes da pravis grafik zavisnosti greske od velicine set-a (learnig curves). Na isti grafik stavis training set i validation set
ako je greska visoka i ako su obe krive blizu, vrv underfitting. Ne pomaze ako dodajes podatke
ako je greska na trainig set-u niska i postoji razmak onda vrv overfitting. Bice bolje ako dodajes podatke

generalization error (odredjuje da li over, under fituje) je suma 3 stvari
Bias: nastaje zbog losih pretpostavki (linearno kad je usvari polinomijalno). Ako je ovo visoko underfitting
Variance: nastaje zbog prevelike osetljivosti na male promene u trening setu. Ako je ovo visoko overfitting
irreducible error: ako su losi podaci (noisy)

Bias i variance su obrnuto proporcijalni

ako model overfituje moras da ga ogranicis, kod polinomijalnih mozes da smanjis stepen, kod linearnih mozes da ogranicis weights

Ridge regression (l2 penalty)
-------------------------------------
na cost funkciju se dodaje alfa * suma kvadrata tezina (reguralization term), ovo ga tera da tezine budu manje
ovaj reguralization term se samo koristi kad treniras, kad ga evaluiras posle treninga nema ovaj term
alfa je hiperparametar, sto veci tezine blize nuli
bitno je da normalizujes pre nego sto radis ovo

Lasso regression (l1 penalty)
--------------------------------------
slicno samo se dodaje alfa * suma apsolutnih vrednosti tezina
lasso generalno eliminise neke faktore

Elastic net
--------------------------------------
kombinacija prosla dva
ima hiperparametar r, koji kontrolise miks izmedju ta 2
generalno neces nikad cistu linearnu regresiju da koristis, ridge je dobar za pocetak
ako mislis da malo feature-a doprinosi onda lasso ili elastic, generalno je bolji elastic. 

Early stopping
---------------------------------------
treniras ga sve dok validation error pada, kada opet pocne da raste zaustivis treniranje
ovo sprecava overfitting
kod mini-batch GD i SGD nije dobra ideja da se stane cim error pocne da raste, jer moze opet da se spusti
mozes da ga prekides kad je error iznad neke min vrednosti odredjeno vreme

Logistic regression
---------------------------------------
veoma slicno klasicnoj regresiji, samo sto kad sracuna onu sumu, ne izbaci to nego ga provuce kroz sigmoid koji vraca broj 
izmedju 0 i 1

sigmoid = 1/(1+exp(-t))

ako je dobijeni broj manji od 0.5 onda pripada jednoj klasi, else pripada drugoj klasi

cost fja: (y je kojoj grupi pripada, p je ono sto dobijes nako sigmoid)
    c = -log(p) ako je y = 1
        -log(1-p) ako je y = 0

ova prva cena jer sto je p manje cena je veca (ako je p bas malo tj. ako je model ubedjen da je y = 0 kazna velika)
drugo ista logika samo obrnuto

ne postoji normal equation, moze gradient descent

softmax regression
------------------------------------------
moze da ima k klasa
radi tako sto ima razdvojene weights i bias za svaku klasu, onda sracuna score za svaku klasu. 
verovatnocu da pripada klasi k dobijes kad provuces score-ove kroz softmax fju
p_k = (exp(s_k))/sum(exp(s_j)) (j od 1 do k). vraca ono sa najvecim score-om

kad trenira pokusava da minimizuje cross entropy


SUPPORT VECTOR MACHINES
--------------------------------------------------------------------------------------------------------------------------------

bas dobri za komplikovane small to medium size dataset
pokusavas da provuces sto veci "put" izmedju klasa
osetljivi na velicinu brojeva, treba da se normalizuje
hard i soft margin classification
hard je lose jer je osetljivo na outlier
hiperparametar c treba da smanjis ako overfituje

SGDClassifier koristi SGD da bi trenira SVM
Ako koristis LinearSVC treba da centriras podatke tako sto ces da oduzmes mean, ako koristi StandardScaler to je vec uradjeno
loss treba da stavis na hinge i dual na false

ako treba Polynomial mozes ono PolynomialFeatures

kernel = "poly" simulira polinomijalno govno bez da ga zapravo radi. 
coef01 utice na to koliko niski stepeni vs visoki uticu na model 

simularity feature:
dodas feature koji meri koliko je uzorak slican nekom landmark-u
kako se bira landmark? najlakse da svaka instanca bude landmark, ako imas m instanca imas i m feature-a

moze kernal = "rbf" ista fora kao za poly
vece gamma, bell curve uzi
ako overfituje smanji gamma

generalno treba prvo da probas LinearSVC i rbf

class         | Time complexity      | Out-of-core support | Scaling required | Kernel trick
---------------------------------------------------------------------------------------------
LinearSVC     | O(m*n)               |  No                 |    Yes           | No
SGDClassifier | O(m*n)               |  Yes                |    Yes           | No
SVC           | O(m^2*n) do O(m^3*n) |  No                 |    Yes           | Yes
---------------------------------------------------------------------------------------------

DESCISION TREES
-----------------------------------------------------------------------------------------------------------
znas sa intsis kako radi
ne moras da normalizujes (scaling)

CART algoritam brani binarna stable, ID3 moze i vise od 2

mogu da daju i verovatnoucu da pripada odredjenoj klasi
radi prvo klasicno do lista, onda deli broj instanca koji pripadaju klasama sa ukupnim brojem instanci koje su u tom listu
(jasnije kad pogledas sliku str. 207(pdf) 181(knjiga))

predikcije brze O(log2(m))
trening spor, jer uporedjuje sve feature-e na svim instancama na svakom cvoru O(n*3*log2(m))

gini vs entropy: nije bas bitno, slicna resenja, gini se brze racuna.
entropy mozda pravi vise balansirana stabla

ako treba regularization kad overfituje:
    max_depth: mozes da stavis max_depth ( max dubina stabla)
    min_tree_split: min broj instanca koji cvor mora da ima da bi se podelila
    min_samples_leaf: min broj instanca koji list mora da ima
    min_weight_fraciton_leaf: isto kao proslo, samo izrazeno kao razlomak
    max_leaf_node: max broj listova
    max_features: max broj feature-a koji se procenjuje za podelu na svakom cvoru

smanjis max_* ili povecas min_*

ENSAMBLE LEARNING AND RANDOM FORESTS
-------------------------------------------------------------------------------------------------------------------
